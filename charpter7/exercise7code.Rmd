---
title: "Slhk8"
author: "zhixue"
date: "5/3/2019"
output:
  pdf_document: default
  html_document: default
---

> 6.In this exercise, you will further analyze the Wage data set considered throughout this chapter.  
(a) Perform polynomial regression to predict wage using age. Use cross-validation to select the optimal degree d for the polyno- mial. What degree was chosen, and how does this compare to the results of hypothesis testing using ANOVA? Make a plot of the resulting polynomial fit to the data.  
(b) Fit a step function to predict wage using age, and perform cross- validation to choose the optimal number of cuts. Make a plot of the fit obtained.

```{r 7.6}
set.seed(0)
library(ISLR)
library(boot)
#(a)
# k-fold
k = 10
deltas = rep(NA, k)
for (i in 1:k){
  glm.fit = glm(wage~poly(age,i), data=Wage)
  # The first component is the raw cross-validation estimate of prediction error. The second component is the adjusted cross-validation estimate. 
  deltas[i] = cv.glm(Wage,glm.fit,K=k)$delta[2]
}
plot(1:k, deltas, xlab="degree", ylab="cv error", type="l") # choose 3 or 4

# anova
fits = list()
for (i in 1:k){
  fits[[i]] = lm(wage~poly(age, i), data=Wage) 
}

anova(fits[[1]], fits[[2]], fits[[3]], fits[[4]], fits[[5]], fits[[6]], fits[[7]], fits[[8]], fits[[9]], fits[[10]]) # choose 3

# resulting polynomial 
# points
plot(wage~age,data=Wage,col='grey')
# model and line
ageranges = range(Wage$age)
age.grid = seq(from=ageranges[1], to=ageranges[2])
lm.fit = lm(wage~poly(age, 3), data=Wage)
lm.pred = predict(lm.fit, data.frame(age=age.grid))
lines(age.grid, lm.pred, col="red")

#(b)
# try cut from 2~10
deltas = rep(NA,k)
for (i in 2:k){
  Wage$tempage = cut(Wage$age,i)
  lm.fit = glm(wage~tempage,data=Wage)
  deltas[i] = cv.glm(Wage,lm.fit,K=k)$delta[2]
}
plot(1:k, deltas, xlab="cut num", ylab="cv error", type="l") # choose 8

# fit
lm.fit = glm(wage~cut(age,8),data=Wage)
ageranges = range(Wage$age)
age.grid = seq(from=ageranges[1],to=ageranges[2])
lm.pred = predict(lm.fit,data.frame(age=age.grid))
plot(wage~age,data=Wage,col='grey')
lines(age.grid,lm.pred,col='red')
```

> 8.Fit some of the non-linear models investigated in this chapter to the Auto data set. Is there evidence for non-linear relationships in this data set? Create some informative plots to justify your answer.

```{r 7.8}
library(ISLR)
library(boot)
set.seed(0)
plot(Auto) # mpg～displacement,horsepower,weight

# try displacement polynomial 
k = 10
deltas = rep(NA,k)
fits = list()
for (i in 1:k){
  fits[[i]] = glm(mpg~poly(displacement,i),data=Auto)
  deltas[i] = cv.glm(Auto, fits[[i]],K=k)$delta[2] 
}
plot(1:k, deltas, xlab="degree", ylab="cv error", type="l") # choose 2 
# anava
anova(fits[[1]], fits[[2]], fits[[3]], fits[[4]], fits[[5]], fits[[6]], fits[[7]], fits[[8]], fits[[9]], fits[[10]]) # choose 2

# try displacement step
# try cut from 2~10
deltas = rep(NA,k)
for (i in 2:k){
  Auto$tempdisplacement = cut(Auto$displacement,i)
  lm.fit = glm(mpg~tempdisplacement,data=Auto)
  deltas[i] = cv.glm(Auto,lm.fit,K=k)$delta[2]
}
plot(1:k, deltas, xlab="cut num", ylab="cv error", type="l") # choose 9

# try spline
library(splines)
k=10
deltas = rep(NA,k)
for (i in 3:k){
  fit = glm(mpg~ns(displacement,df=i), data=Auto)
  deltas[i] = cv.glm(Auto,fit,K=k)$delta[2]
}
plot(1:k, deltas, xlab="degree", ylab="cv error", type="l") # choose 9

# try gam
#install.packages('gam')
library(gam)
fit = gam(mpg ~ s(displacement, 4) + s(horsepower, 4) +s(weight,4), data = Auto)
summary(fit)
```

>10. This question relates to the College data set.  
(a) Split the data into a training set and a test set. Using out-of-state tuition as the response and the other variables as the predictors, perform forward stepwise selection on the training set in order to identify a satisfactory model that uses just a subset of the predictors.  
(b) Fit a GAM on the training data, using out-of-state tuition as the response and the features selected in the previous step as the predictors. Plot the results, and explain your findings.  
(c) Evaluate the model obtained on the test set, and explain the results obtained.  
(d) For which variables, if any, is there evidence of a non-linear relationship with the response?  

```{R 7.10}
set.seed(0)
library(ISLR)
#install.packages('leaps')
library(leaps)
attach(College)
# a
# out-of-state tuition as the response and the other variables as the predictors
train = sample(length(Outstate), length(Outstate)/2)
test = -train
College.train = College[train, ]
College.test = College[test, ]
# forward stepwise selection
reg.fit = regsubsets(Outstate ~ ., data = College.train, nvmax = 17, method = "forward")
reg.summary = summary(reg.fit)
# plot cp,bic,adjr2
par(mfrow = c(1, 3))
plot(reg.summary$cp, xlab = "Number of Variables", ylab = "Cp", type = "l")
plot(reg.summary$bic, xlab = "Number of Variables", ylab = "BIC", type = "l")
plot(reg.summary$adjr2, xlab = "Number of Variables", ylab = "Adjusted R2",type = "l")
which.min(reg.summary$cp)
which.min(reg.summary$bic)
which.min(reg.summary$adjr2)

reg.fit = regsubsets(Outstate ~ ., data = College, method = "forward")
coefi = coef(reg.fit, id = 6)
names(coefi)

# b Outstate increases when private is yes/ room.board increases/ phd increases/ perc.alumni increases/ expend increases/ grad.rate increases. 
gam.fit = gam(Outstate ~ Private + s(Room.Board, df = 2) + s(PhD, df = 2) + s(perc.alumni, df = 2) + s(Expend, df = 5) + s(Grad.Rate, df = 2), data = College.train)
par(mfrow = c(2, 3))
plot(gam.fit, se = T, col = "red")

# c 
gam.pred = predict(gam.fit, College.test)
gam.err = mean((College.test$Outstate - gam.pred)^2)
gam.err

gam.tss = mean((College.test$Outstate - mean(College.test$Outstate))^2)
test.rss = 1 - gam.err/gam.tss
test.rss # test r^2 = 0.77495

# d Expand(***),Room.Board(*),perc.alumni(*)
summary(gam.fit)
```

>12. This problem is a continuation of the previous exercise. In a toy example with p = 100, show that one can approximate the multiple linear regression coefficient estimates by repeatedly performing simple linear regression in a backfitting procedure. How many backfitting iterations are required in order to obtain a “good” approximation to the multiple regression coefficient estimates? Create a plot to justify your answer.

```{R 7.12}
par(mfrow = c(1, 1))
# random
set.seed(0)
p = 100
n = 1000
x = matrix(ncol = p, nrow = n)
coefi = rep(NA, p)
for (i in 1:p) {
    x[, i] = rnorm(n)
    coefi[i] = rnorm(1) * 100
}
y = x %*% coefi + rnorm(n)

# backfit
betas = rep(0,p)
MAX_ITER = 500
errors = rep(NA,MAX_ITER+1)
# init
iter = 2
errors[1] = Inf
errors[2] = sum((y- x %*% betas)^2)
threshold = 0.001
# start
while(iter < MAX_ITER && errors[iter-1] - errors[iter] > threshold){
  for (i in 1:p){
    # update betas
    temp = y - x %*% betas + betas[i] * x[,i]
    betas[i] = lm(temp~x[,i])$coef[2]
  }
  iter = iter + 1
  errors[iter] = sum((y-x %*% betas)^2)
}
# near 11 iterations
plot(3:iter-2,errors[3:iter], type="l")
```
